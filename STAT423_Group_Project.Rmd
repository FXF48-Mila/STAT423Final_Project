---
title: "STAT423_Group_Project"
output: html_document
date: "2024-02-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(MASS)
library(caret)
library(faraway)
library(tidyr)
library(dplyr)
library(ggplot2)
library(patchwork)
```

# Introduction

Health insurance plays a crucial role in safeguarding individuals and families against the financial burdens associated with healthcare expenses. Especially after Covid-19, "health insurance has become one of the most prominent areas of research." \footnote{(Health Insurance Cost Prediction Using Machine Learning, n.d.)} As medical costs continue to rise and healthcare needs evolve, health insurance charges also increase gradually, after a 4% increase in health insurance in 2023, "American families will face another 4% increase in the cost of private health insurance." \footnote{(ValuePenguin, 2023)} People can't help asking after reading these exposures — How do health insurance charges?

This report sets out on an exploratory voyage in an effort to solve the mystery surrounding health insurance charges and then give forecast models. The rest of the report is divided into data description, models, results analysis, and conclusions. Through these sections, this report aims to shed light on the complex interplay of factors that have contributed to health insurance charges. By delving into these topics, we aim to unravel the factors that influence insurance charges and to fit the linear regression models capable of accurately predicting health insurance charges.

# Data
## About The Data

```{r}
data <- read.csv("Health_insurance.csv")
```

```{r data_check}
head(data)
# skimr::skim(data)
```

The dataset is collected by Kaggle. \footnote{(Health Insurance Dataset, 2020)} The dataset contains 1338 observations across 7 columns of customer personal information: age, sex(female or male), bmi(body mass index), children(the number of children), smoker(yes or no), regions(customers’ residential area), and charges (insurance charge bill).  Specifically, there are 3 character columns: `sex`, `smoker`, `region`, and 4 numeric columns: `age`, `children`, `bmi`, and `charges`, providing a comprehensive foundation for analysis and exploration of health insurance-related factors.

## Data Prepocessing

```{r}
data$sex <- as.factor(data$sex)
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)
```

In order to address the issue of having multiple intercepts due to the presence of categorical variables in regression analysis, we change `sex`, `smoker`, and `region` into factors in R. 

## Data Split 
```{r data_split_brefore_transformation}
set.seed(423)

df_shuffled <- data[sample(nrow(data)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set1 <- df_shuffled[1:n_train, ]
test_set1 <- df_shuffled[(n_train + 1):n_total, ]
```

In order to facilitate the development and evaluation of predictive models, we divided the dataset into 80% training and 20% testing subsets. The training set, comprising 80% of the data, used to construct and refine our models in the following sections.

## Data Processing -- Log Transformation

```{r fig.height=4, fig.width=5.5}
histogram(train_set1$charges, xlab = "Health Insurance Charges", main = "The Histogram for Health Insurance Charges")
```

We can see the distribution of Health Insurance Charges is very right skewed. In orde to fit the full regression model, we need a Box-Cox transformation.

```{r box-cox_transform, fig.height=4, fig.width=5.5}
# Box-Cox transformation
bc_transform <- boxcox(charges ~ ., data = train_set1, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Box-Cox transform.")
```

```{r log_transform, fig.height=4, fig.width=5.5}
# Log transformation
train_set1$log_charges <- log(train_set1$charges)
histogram(train_set1$log_charges, xlab = "Log Health Insurance Charges", main = "The Histogram for Log Health Insurance Charges")

```

## Fit Full Model And Assumption Checks

We then want to fit a full model to see how personal information is related to the charges. However, we need to check the assumption first.

```{r}
fit.full <- lm(log_charges ~ age + sex + bmi + children + smoker + region, data = train_set1)
#summary(fit.full)
```

```{r full_assumption}
par(mfrow=c(2, 2))
plot(fit.full,which=1)
plot(fit.full,which=2)
plot(fit.full,which=3)
plot(fit.full,which=4)
```

In the TA plot, the points are not randomly around the horizontal line at zero. And there exists an increasing and decreasing trend at the beginning and the ending respectively, which suggests that the linearity assumption is violated. In the Scale-Location plot, the spread of points is not constant across all values of the fitted values, which indicates that the assumption of homoscedasticity is violated. In the residuals plot, the last part of the quantile deviates a lot, indicating the residuals are not normally distributed, violating the assumption of normality. In the Residuals vs. Leverage plot, there exist three points that have high leverage values (typically exceeding twice the average leverage). These three points are considered influential and may lead to some misunderstanding in the following investigation.

Therefore, we can see that almost every assumption is violated in this case even though the explanatory variables are normal distributed.

```{r}
plot(train_set1)
```

We can clearly see that there exist two regression lines in diagnostic plots for `age` and `log_charges`.

## Split The Dataset Based on Smoke Status

Based on all the observations above, smoking status may have a significant impact on insurance charges. Therefore, we decided to split the dataset into smoker and nonsmoker groups to facilitate more accurate predictions. However, we realize that splitting the dataset based on smoking status may alter the distribution of the response variable: charges. Therefore, we are going to transform the dataset separately for each smoke status group to ensure our modeling is appropriate for these differences and then produce a reliable analysis.
 
### Smoker

```{r}
train_set1 <- train_set1 %>%
  mutate(children = ifelse(children > 0, "have child", "no child"))
```

The potential issue regarding the dataset is the variable representing the number of children. To be more specific, the dataset tends to be a higher frequency of individuals with fewer children (0, 1, or 2) compared to those with a larger number of children (3, 4, or 5). This can lead to imbalanced data distribution and potentially affect the accuracy of statistical analyses, such as regression modeling.

In order to address this issue and ensure a more balanced representation of the data, we transform the numerical variable representing the number of children into a categorical variable with distinct groups: "no child," and "have child." This categorization helps to address the potential skewness in the data distribution and improves the interpretability of regression analysis results.

### Non-Smoker


# Method


```{r}
plot(train_set1)
```

```{r data_check_og}
par(mfrow=c(2,2))

hist(train_set1$age, main="Age Distribution", xlab="Age")
hist(train_set1$bmi, main="BMI Distribution", xlab="BMI")
hist(train_set1$charges, main="Charges Distribution", xlab="Charges")
```
```{r lr_fit_initial}
fitfull <- lm(charges ~ ., data = train_set1)
summary(fitfull)
```
```{r detailed_residual_plot}
par(mfrow=c(2, 2))

plot(fitfull,which=1)
plot(fitfull,which=2)
plot(fitfull,which=3)
plot(fitfull,which=4)
```
```{r, boxcox}
datasmoker <- train_set1 %>% filter(smoker == "yes")
datasmoker <- datasmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datasmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
datasmoker$charges <- (datasmoker$charges^lambda_optimal - 1) / lambda_optimal

test_smoker <- test_set1 %>% filter(smoker == "yes")
test_smoker <- test_smoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_smoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
test_smoker$charges <- (test_smoker$charges^lambda_optimal - 1) / lambda_optimal

datanosmoker <- train_set1 %>% filter(smoker == "no")
datanosmoker <- datanosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datanosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Non-Smoker Box-Cox transform.")
datanosmoker$charges <- log(datanosmoker$charges)

test_nosmoker <- test_set1 %>% filter(smoker == "no")
test_nosmoker <- test_nosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_nosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Non-Smoker Box-Cox transform.")
test_nosmoker$charges <- log(test_nosmoker$charges)

```

```{r}
fit12 <- lm(charges ~ age+sex+bmi+children+region, data = datasmoker)
summary(fit12)
par(mfrow=c(2, 2))
plot(fit12,which=1)
plot(fit12,which=2)
plot(fit12,which=3)
plot(fit12,which=4)
```

```{r}
fit13 <- lm(charges ~ age+sex+bmi+children+region, data = datanosmoker)
summary(fit13)
par(mfrow=c(2, 2))
plot(fit13,which=1)
plot(fit13,which=2)
plot(fit13,which=3)
plot(fit13,which=4)
```

## Model Selection

The Mallow’s Cp statistic, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used for the appropriate model selection. While Mallow’s Cp assesses the trade-off between model fit and complexity in linear regression, AIC and BIC extend to various modeling contexts, penalizing complexity differently based on information theory and sample size, respectively. Here, we have chosen to use these three model selection criteria for all the possible models (including the interactions of all variables) that we can build from the dataset. The surprising result is that all the optimal model selection criteria ultimately point to the same model with parameters `bmi` , `age` , and `age:bmi` interaction. The corresponding Mallow's Cp statistic, AIC, and BIC for this selected model are displayed in the following table.

```{r}
criteria_statistics <- function(mdl.list) {
  n <- nobs(mdl.list[[1]])
  DoFs <- sapply(mdl.list, function(mdl) { sum(hatvalues(mdl)) })
  MSEs <- sapply(mdl.list, function(mdl) { mean(residuals(mdl)^2) })
  AIC_values <- sapply(mdl.list, function(mdl) { AIC(mdl, k = 2) }) 
  BIC_values <- sapply(mdl.list, function(mdl) { BIC(mdl) })
  
  biggest <- which.max(DoFs)
  sigma2.hat <- MSEs[[biggest]]*n/(n-DoFs[[biggest]])
  Cp <- MSEs + 2*sigma2.hat*DoFs/n
  
  criteria_values <- cbind(Cp = Cp,
                           AIC = AIC_values,
                           BIC = BIC_values)
  return(criteria_values)
}

find_best_model <- function(criteria_values, criterion) {
  best_model_index <- which.min(criteria_values[, criterion])
  best_model <- models[[best_model_index]]
  best_combination <- combinations[[best_model_index]]
  best_criteria_value <- criteria_values[best_model_index, criterion]
  return(list(model = best_model, combination = best_combination, value = best_criteria_value))
}

get_combinations <- function(vars) {
  lapply(1:length(vars), function(x) combn(vars, x, simplify = FALSE))
}

input_vars <- names(data)[!names(data) %in% c("charges", "smoker")]

combinations <- unlist(get_combinations(input_vars), recursive = FALSE)

create_models <- function(data, combinations) {
  lapply(combinations, function(vars) {
    formula <- as.formula(paste("charges ~", paste(vars, collapse = "*")))
    lm(formula, data = datasmoker)
  })
}

models <- create_models(datasmoker, combinations)

criteria_values <- criteria_statistics(models)

best_model_Cp <- find_best_model(criteria_values, "Cp")
best_model_AIC <- find_best_model(criteria_values, "AIC")
best_model_BIC <- find_best_model(criteria_values, "BIC")

print("Best Model based on Cp:")
print(best_model_Cp$model)
print(best_model_Cp$value)
print("")

print("Best Model based on AIC:")
print(best_model_AIC$model)
print(best_model_AIC$value)
print("")

print("Best Model based on BIC:")
print(best_model_BIC$model)
print(best_model_BIC$value)
```

| Mallow's Cp| AIC | BIC |
|----------|----------|----------|
| 0.03973949 | -81.53324 | -64.65685  |

## Analysis of the selected linear model



```{r}
fit_choice <- lm(formula = charges ~ bmi + age + age:bmi, data = datasmoker)
summary(fit_choice)
```

## Assumption Checks

```{r}
par(mfrow=c(2, 2))
plot(fit_choice,which=3)
plot(fit_choice,which=2)
plot(fit_choice,which=4)
plot(fit_choice,which=5)
```

```{r}
vif(fit_choice)
```


## Prediction

```{r}
fit_choice <- lm(formula = charges ~ bmi + age + age:bmi, data = test_smoker)
data_for_plot <- data.frame(
  Actual = test_smoker$charges,
  LR_Selected_Predicted = predict(fit_choice, newdata = test_smoker)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

# Visualizing the results
ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```

For the selected model $charges=41.3066179+8.1802807bmi+1.5338854age-0.0009364bmi:age$, the only significant predictors is bmi. The predictor age and interaction term bmi:age are not statistically significant, which implies that the effect of age and the interaction effect between 'bmi' and 'age' on charges are not evident from the data. The F-statistic 220.3 with a significantly small p-value suggests that the overall model is statistically significant. The residual standard error is 33.76 and the adjusted R-squared value is 0.7537, which shows the proportion of the variance for the dependent variable explained by predictor
variables. 

The scatterplot shows the relationship between the actual charge and the charge predicted by the selected linear regression model, represented by the red dots. The data points are distributed around the dashed line, which represents the perfect prediction line, where the actual value is equal to the predicted value. The linear regression model seems to perform reasonablely well, as many of the points are clustered around the perfect fit line. However, there are significant deviations from this line, indicating some degree of prediction error. Prior to the actual charges reaching 350, the model seems to predict values that are higher than the actual figures, indicating an overestimation. Conversely, after the actual charges surpass 350, the model's predictions tend to be lower, suggesting an underestimation.




## Non-Smoker


```{r}
# data <- read.csv("Health_insurance.csv")
# data$sex <- as.factor(data$sex)
# data$smoker <- as.factor(data$smoker)
# data$region <- as.factor(data$region)
# 
# data$charges <- log(data$charges)

non_smoker_data <- datanosmoker

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)

threshold <- quantile(abs(residuals), 0.85)

data_with_small_residuals <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r}
par(mfrow=c(2, 2))
plot(initial_model,which=1)
plot(initial_model,which=2)
plot(initial_model,which=3)
plot(initial_model,which=4)
```

As we can see from the graphs, the data of non-smoker is showing a clear pattern of residuals that is dense on the bottom of the residual plots and also some outliers that lies above it. Therefore, we would consider to extract this pattern out of the data and consider those floating data to be outliers in order to make a linear regression model. Consider the residuals that
$$\hat\epsilon_i=Y_i-\hat Y_i=Y_i-\hat\beta_0-\hat\beta_1X_i.$$
If we compare the residuals to $\hat{\epsilon}_i$ in the regression model:
$$\hat{Y}_i = \beta_0 + \beta_1X_i + \epsilon_i \quad \Longleftrightarrow \quad \epsilon_i = Y_i - \beta_0 - \beta_1X_i.$$
Essentially, each $e_i$ mimics the role of $\hat{\epsilon}_i$ when the fitted coefficients $\hat{\beta}_0, \hat{\beta}_1$ are close to $\beta_0, \beta_1$. Therefore, we would consider to use $\hat\epsilon_i$ of the linear model to capture the distance of the data point to the assumed model and therefore only keep data that are not outliers. In this case we choose 80% quantile of the empirical residuals that 
$$\hat Y=\{\hat Y_i:\hat\epsilon_i<Q_{0.85\hat\epsilon}\}.$$
To visualize these data point, we can draw following graphs:

```{r}
ggplot(non_smoker_data, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Original Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(data_with_small_residuals, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Extracted Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(data_outliers, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Age vs Charges Outliers", x = "Age", y = "Charges") +
  theme_minimal()
```

Therefore, we would consider that we have successfully deleted the outliers that are irrelevant to our model. Therefore, the linear regression model would be:

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = data_with_small_residuals)
summary(new_model)
```
As we can see from the table given from `lm()` function, all the factors except BMI are significant. 

```{r}
par(mfrow=c(2, 2))
plot(new_model,which=1)
plot(new_model,which=2)
plot(new_model,which=3)
plot(new_model,which=4)
```

```{r}
sampled_residuals <- sample(resid(new_model), replace = TRUE)
new_response <- fitted(new_model) + sampled_residuals
bootstrap_data <- data_with_small_residuals
bootstrap_data$charges <- new_response

model2<-lm(charges ~ age+sex+bmi+children+region, data = bootstrap_data)
```

```{r}
par(mfrow=c(2, 2))
plot(model2,which=1)
plot(model2,which=2)
plot(model2,which=3)
plot(model2,which=4)
```

```{r}
# non_smoker_data <- filter(data, smoker == "no")

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)
threshold <- quantile(abs(residuals), 0.85)

data_used <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r data_split2}
set.seed(423)

df_shuffled <- data_used[sample(nrow(data_used)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set <- df_shuffled[1:n_train, ]
test_set <- df_shuffled[(n_train + 1):n_total, ]
```

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = train_set)

set.seed(423)
bootstrap_coefficients <- replicate(1000, {
  sampled_residuals <- sample(resid(new_model), replace = TRUE)
  new_response <- fitted(new_model) + sampled_residuals
  bootstrap_data <- train_set
  bootstrap_data$charges <- new_response
  coef(lm(charges ~ age + sex + bmi + children + region, data = bootstrap_data))
}, simplify = "matrix")

bootstrap_coefficients_df <- as.data.frame(t(bootstrap_coefficients))
colnames(bootstrap_coefficients_df) <- names(coef(initial_model))

bootstrap_summary <- summary(bootstrap_coefficients_df)

print(bootstrap_summary)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
ci_df <- data.frame(lower = ci_lower, upper = ci_upper)
print(ci_df)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
coefficients_df <- data.frame(
  term = names(means),
  estimate = means,
  ci_lower = ci_lower,
  ci_upper = ci_upper
)

coefficients_df <- coefficients_df[-1, ]

p <- ggplot(coefficients_df, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  theme_minimal() +
  labs(x = "Term", y = "Estimate", title = "Confidence Intervals for Model Coefficients") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(p)
```

The plot shows confidence intervals for model coefficients from a regression analysis for non-smoker data by using the bootstrap method. In the model, the predictors age and children have positive coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. While predictors regionnorthwest, regionsoutheast and regionsouthwest have negative coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. The coefficient of bmi has small confidence interval and it contains 0, which indicates it is not a significant predictor in the model. The result is consistent to the result from previous `lm()` table.


```{r}
beta_0 <- means[1]
beta_age <- means[2]
beta_sex_male <- means[3]
beta_bmi <- means[4]
beta_children <- means[5]
beta_region_northwest <- means[6]
beta_region_southeast <- means[7]
beta_region_northeast <- means[8]

predict_charges <- function(new_data) {
  beta_0 +
    beta_age * new_data$age +
    beta_sex_male * as.numeric(new_data$sex == 'male') +
    beta_bmi * new_data$bmi +
    beta_children * new_data$children  +
    beta_region_northwest * as.numeric(new_data$region == 'northwest') +
    beta_region_southeast * as.numeric(new_data$region == 'southeast') +
    beta_region_northeast * as.numeric(new_data$region == 'northeast')
}
```

```{r}
data_for_plot <- data.frame(
  Actual = test_set$charges,
  LR_Predicted = predict_charges(test_set)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```


The linear regression model appears predict the charges well, as indicated by the close clustering of the data points around the line of perfect prediction, where the actual value is equal to the predicted value. The data points are represented by pink dots, where the x-axis shows the actual charges and the y-axis shows the charges predicted by the model. However, for actual charges between 7.5 and 8, the model appears to overpredict, indicated by the points lying above the dashed line. As the actual charges rise from 8 to 9, the model seems to underpredict, with points lying below the dashed line. Overall, the prediction aligns well with the expected outcomes, with no apparent outlier present.


# References

Health Insurance dataset. (2020, December 18). https://www.kaggle.com/datasets/shivadumnawar/health-insurance-dataset/data

K. Bhatia, S. S. Gill, N. Kamboj, M. Kumar and R. K. Bhatia, "Health Insurance Cost Prediction using Machine Learning," 2022 3rd International Conference for Emerging Technology (INCET), Belgaum, India, 2022, pp. 1-5, doi: 10.1109/INCET54531.2022.9824201. keywords: {COVID-19;Pediatrics;Obesity;Costs;Correlation;Pandemics;Linear regression;Health Insurance;Cost Prediction;Machine learning;Regression},

ValuePenguin. (2023, December 19). Private health insurance premiums reach a record high of $7008/year in 2024. PR Newswire: press release distribution, targeting, monitoring and marketing. https://www.prnewswire.com/news-releases/private-health-insurance-premiums-reach-a-record-high-of-7008year-in-2024-302019327.html#:~:text=After%20a%200.67%25%20increase%20in,for%20healthcare%20providers%20and%20insurers 
