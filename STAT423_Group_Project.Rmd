---
title: "STAT423_Group_Project"
output: html_document
date: "2024-02-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(MASS)
library(caret)
library(faraway)
library(tidyr)
library(dplyr)
library(ggplot2)
```
# Introduction

# Data
## About the Data

```{r}
data <- read.csv("Health_insurance.csv")
```

```{r data_check}
head(data)
skimr::skim(data)
```

## Data Prepocessing
```{r}
# data <- data %>%
#   mutate(children = ifelse(children > 0, "have child", "no child"))
data$sex <- as.factor(data$sex)
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)
```

## Data Split (80% training 20% testing)
### (Split Before Transformation)
```{r data_split_brefore_transformation}
set.seed(423)

df_shuffled <- data[sample(nrow(data)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set1 <- df_shuffled[1:n_train, ]
test_set1 <- df_shuffled[(n_train + 1):n_total, ]
```

## Data Transformation(Log)

```{r box-cox_transform}
# Box-Cox transformation
bc_transform <- boxcox(charges ~ ., data = data, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Box-Cox transform.")

# Log transformation
data$charges <- log(data$charges)
```

## Fit Full Model

## Divide the dataset to smoker and non-smoker


# Method


```{r}
plot(train_set1)
```

```{r data_check_og}
par(mfrow=c(2,2))

hist(train_set1$age, main="Age Distribution", xlab="Age")
hist(train_set1$bmi, main="BMI Distribution", xlab="BMI")
hist(train_set1$charges, main="Charges Distribution", xlab="Charges")
```
```{r lr_fit_initial}
train_setinitial <- train_set1
train_setinitial$charges <- log(train_setinitial$charges)
fitfull <- lm(charges ~ ., data = train_setinitial)
summary(fitfull)
```
```{r detailed_residual_plot}
par(mfrow=c(2, 2))

plot(fitfull,which=1)
plot(fitfull,which=2)
plot(fitfull,which=3)
plot(fitfull,which=4)
```
```{r, boxcox}
datanosmoker <- train_set1 %>% filter(smoker == "no")
datanosmoker <- datanosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datanosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Non-Smoker Box-Cox transform.")
datanosmoker$charges <- log(datanosmoker$charges)

test_nosmoker <- test_set1 %>% filter(smoker == "no")
test_nosmoker <- test_nosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_nosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Non-Smoker Box-Cox transform.")
test_nosmoker$charges <- log(test_nosmoker$charges)

train_set1 <- train_set1 %>%
  mutate(children = ifelse(children > 0, "have child", "no child"))

datasmoker <- train_set1 %>% filter(smoker == "yes")
datasmoker <- datasmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datasmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
datasmoker$charges <- (datasmoker$charges^lambda_optimal - 1) / lambda_optimal

test_smoker <- test_set1 %>% filter(smoker == "yes")
test_smoker <- test_smoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_smoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
test_smoker$charges <- (test_smoker$charges^lambda_optimal - 1) / lambda_optimal
```


```{r}
fit12 <- lm(charges ~ age+sex+bmi+children+region, data = datasmoker)
summary(fit12)
par(mfrow=c(2, 2))
plot(fit12,which=1)
plot(fit12,which=2)
plot(fit12,which=3)
plot(fit12,which=5)
```

```{r}
fit13 <- lm(charges ~ age+sex+bmi+children+region, data = datanosmoker)
summary(fit13)
par(mfrow=c(2, 2))
plot(fit13,which=3)
plot(fit13,which=2)
plot(fit13,which=4)
plot(fit13,which=5)
```


## Model Selection

The Mallow’s Cp statistic, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used for the appropriate model selection. While Mallow’s Cp assesses the trade-off between model fit and complexity in linear regression, AIC and BIC extend to various modeling contexts, penalizing complexity differently based on information theory and sample size, respectively. Here, we have chosen to use these three model selection criteria for all the possible models (including the interactions of all variables) that we can build from the dataset. The surprising result is that all the optimal model selection criteria ultimately point to the same model with parameters `bmi` , `age` , and `age:bmi` interaction. The corresponding Mallow's Cp statistic, AIC, and BIC for this selected model are displayed in the following table.

```{r}
criteria_statistics <- function(mdl.list) {
  n <- nobs(mdl.list[[1]])
  DoFs <- sapply(mdl.list, function(mdl) { sum(hatvalues(mdl)) })
  MSEs <- sapply(mdl.list, function(mdl) { mean(residuals(mdl)^2) })
  AIC_values <- sapply(mdl.list, function(mdl) { AIC(mdl, k = 2) }) 
  BIC_values <- sapply(mdl.list, function(mdl) { BIC(mdl) })
  
  biggest <- which.max(DoFs)
  sigma2.hat <- MSEs[[biggest]]*n/(n-DoFs[[biggest]])
  Cp <- MSEs + 2*sigma2.hat*DoFs/n
  
  criteria_values <- cbind(Cp = Cp,
                           AIC = AIC_values,
                           BIC = BIC_values)
  return(criteria_values)
}

find_best_model <- function(criteria_values, criterion) {
  best_model_index <- which.min(criteria_values[, criterion])
  best_model <- models[[best_model_index]]
  best_combination <- combinations[[best_model_index]]
  best_criteria_value <- criteria_values[best_model_index, criterion]
  return(list(model = best_model, combination = best_combination, value = best_criteria_value))
}

get_combinations <- function(vars) {
  lapply(1:length(vars), function(x) combn(vars, x, simplify = FALSE))
}

input_vars <- names(data)[!names(data) %in% c("charges", "smoker", "age", "bmi")]

combinations <- unlist(get_combinations(input_vars), recursive = FALSE)

create_models <- function(data, combinations) {
  lapply(combinations, function(vars) {
    formula <- as.formula(paste("charges ~ age + bmi +", paste(vars, collapse = "*")))
    lm(formula, data = datasmoker)
  })
}

models <- create_models(datasmoker, combinations)

criteria_values <- criteria_statistics(models)

best_model_Cp <- find_best_model(criteria_values, "Cp")
best_model_AIC <- find_best_model(criteria_values, "AIC")
best_model_BIC <- find_best_model(criteria_values, "BIC")

print("Best Model based on Cp:")
print(best_model_Cp$model)
print(best_model_Cp$value)
print("")

print("Best Model based on AIC:")
print(best_model_AIC$model)
print(best_model_AIC$value)
print("")

print("Best Model based on BIC:")
print(best_model_BIC$model)
print(best_model_BIC$value)
```

| Mallow's Cp| AIC | BIC |
|----------|----------|----------|
| 1160.439  | 2139.315 | 2156.191  |

## Analysis of the selected linear model

For the selected model $\sqrt{charges}=41.3066179+8.1802807bmi+1.5338854age-0.0009364bmi:age$.

Here, the intercept is not worthy for explaination because it suggests that when both BMI and age are zero, the square root of charges is expected to be approximately 41.3066179. However, no one will have a bmi or age of zero.

According to the summary, for every one unit increase in bmi, the estimate of the average square root of charges increase 8.1802807. For every one increase in age, the estimate of the average square root of charges increase 1.5338854. While for every one unit increase in the product of BMI and age, the square root of charges is expected to decrease by approximately 0.0009364.


Here, the only significant predictors is `bmi`. The predictor age and interaction term bmi:age are not statistically significant, which implies that the effect of age and the interaction effect between 'bmi' and 'age' on charges are not evident from the data.

The F-statistic 220.3 with a significantly small p-value suggests that the overall model is statistically significant. The residual standard error is 33.76 and the adjusted R-squared value is 0.7537, which shows the proportion of the variance for the dependent variable explained by predictor
variables. 

```{r}
fit_choice <- lm(formula = charges ~ bmi + age + sex, data = datasmoker)
summary(fit_choice)
```

## Assumption Checks

The residual vs. fitted plots imply the expected value of residuals is roughly 0. Since we did a square root transformation on the response variable, here, we use the standard residuals vs. fitted plot to check the homoscedasticity. Here, since the spread of the points in the plot remains relatively constant across the range of fitted values, then constant variance assumption is satisfied. The QQ plot seems to satisfy the normality assumption since the majority of the data points seem to lie reasonably close to the line, suggesting that the standardized residuals have a distribution that is close to normal. For the Residuals vs. Leverage plot, since there isn't any point with a cook distance greater than 0.5, there is no noticeable point with high cook’s distance and leverage.

We can see that the standard error for all the parameters in the full model are all small. Thus, there should not be serious multicollinearity
problem for this model. Let's check the multicollinearity for each variable by calculating the vif.

According to the vif that we have calculated for the model, we can see that all of the vif are smaller than 5, meaning that there isn't serious multicollinearity for our parameters. 


```{r}
par(mfrow=c(2, 2))
plot(fit_choice,which=1)
plot(fit_choice,which=2)
plot(fit_choice,which=3)
plot(fit_choice,which=5)
```

```{r}
vif(fit_choice)
```


## Prediction

```{r}
fit_choice <- lm(formula = charges ~ bmi + age + sex, data = test_smoker)
data_for_plot <- data.frame(
  Actual = test_smoker$charges,
  LR_Selected_Predicted = predict(fit_choice, newdata = test_smoker)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

# Visualizing the results
ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```


The scatterplot shows the relationship between the actual charge and the charge predicted by the selected linear regression model, represented by the red dots. The data points are distributed around the dashed line, which represents the perfect prediction line, where the actual value is equal to the predicted value. The linear regression model seems to perform reasonablely well, as many of the points are clustered around the perfect fit line. However, there are significant deviations from this line, indicating some degree of prediction error. Prior to the actual charges reaching 350, the model seems to predict values that are higher than the actual figures, indicating an overestimation. Conversely, after the actual charges surpass 350, the model's predictions tend to be lower, suggesting an underestimation.




## Non-Smoker


```{r}
# data <- read.csv("Health_insurance.csv")
# data$sex <- as.factor(data$sex)
# data$smoker <- as.factor(data$smoker)
# data$region <- as.factor(data$region)
# 
# data$charges <- log(data$charges)

non_smoker_data <- datanosmoker

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)

threshold <- quantile(abs(residuals), 0.85)

data_with_small_residuals <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r}
par(mfrow=c(2, 2))
plot(initial_model,which=1)
plot(initial_model,which=2)
plot(initial_model,which=3)
plot(initial_model,which=4)
```

As we can see from the graphs, the data of non-smoker is showing a clear pattern of residuals that is dense on the bottom of the residual plots and also some outliers that lies above it. Therefore, we would consider to extract this pattern out of the data and consider those floating data to be outliers in order to make a linear regression model. Consider the residuals that
$$\hat\epsilon_i=Y_i-\hat Y_i=Y_i-\hat\beta_0-\hat\beta_1X_i.$$
If we compare the residuals to $\hat{\epsilon}_i$ in the regression model:
$$\hat{Y}_i = \beta_0 + \beta_1X_i + \epsilon_i \quad \Longleftrightarrow \quad \epsilon_i = Y_i - \beta_0 - \beta_1X_i.$$
Essentially, each $e_i$ mimics the role of $\hat{\epsilon}_i$ when the fitted coefficients $\hat{\beta}_0, \hat{\beta}_1$ are close to $\beta_0, \beta_1$. Therefore, we would consider to use $\hat\epsilon_i$ of the linear model to capture the distance of the data point to the assumed model and therefore only keep data that are not outliers. In this case we choose 80% quantile of the empirical residuals that 
$$\hat Y=\{\hat Y_i:\hat\epsilon_i<Q_{0.85\hat\epsilon}\}.$$
To visualize these data point, we can draw following graphs:

```{r}
ggplot(non_smoker_data, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Original Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(data_with_small_residuals, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Extracted Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(data_outliers, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Age vs Charges Outliers", x = "Age", y = "Charges") +
  theme_minimal()
```

Therefore, we would consider that we have successfully deleted the outliers that are irrelevant to our model. Therefore, the linear regression model would be:

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = data_with_small_residuals)
summary(new_model)
```
As we can see from the table given from `lm()` function, all the factors except BMI are significant. 

```{r}
par(mfrow=c(2, 2))
plot(new_model,which=1)
plot(new_model,which=2)
plot(new_model,which=3)
plot(new_model,which=4)
```

```{r}
sampled_residuals <- sample(resid(new_model), replace = TRUE)
new_response <- fitted(new_model) + sampled_residuals
bootstrap_data <- data_with_small_residuals
bootstrap_data$charges <- new_response

model2<-lm(charges ~ age+sex+bmi+children+region, data = bootstrap_data)
```

```{r}
par(mfrow=c(2, 2))
plot(model2,which=1)
plot(model2,which=2)
plot(model2,which=3)
plot(model2,which=4)
```

```{r}
# non_smoker_data <- filter(data, smoker == "no")

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)
threshold <- quantile(abs(residuals), 0.85)

data_used <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r data_split2}
set.seed(423)

df_shuffled <- data_used[sample(nrow(data_used)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set <- df_shuffled[1:n_train, ]
test_set <- df_shuffled[(n_train + 1):n_total, ]
```

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = train_set)

set.seed(423)
bootstrap_coefficients <- replicate(1000, {
  sampled_residuals <- sample(resid(new_model), replace = TRUE)
  new_response <- fitted(new_model) + sampled_residuals
  bootstrap_data <- train_set
  bootstrap_data$charges <- new_response
  coef(lm(charges ~ age + sex + bmi + children + region, data = bootstrap_data))
}, simplify = "matrix")

bootstrap_coefficients_df <- as.data.frame(t(bootstrap_coefficients))
colnames(bootstrap_coefficients_df) <- names(coef(initial_model))

bootstrap_summary <- summary(bootstrap_coefficients_df)

print(bootstrap_summary)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
ci_df <- data.frame(lower = ci_lower, upper = ci_upper)
print(ci_df)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
coefficients_df <- data.frame(
  term = names(means),
  estimate = means,
  ci_lower = ci_lower,
  ci_upper = ci_upper
)

coefficients_df <- coefficients_df[-1, ]

p <- ggplot(coefficients_df, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  theme_minimal() +
  labs(x = "Term", y = "Estimate", title = "Confidence Intervals for Model Coefficients") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(p)
```

The plot shows confidence intervals for model coefficients from a regression analysis for non-smoker data by using the bootstrap method. In the model, the predictors age and children have positive coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. While predictors regionnorthwest, regionsoutheast and regionsouthwest have negative coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. The coefficient of bmi has small confidence interval and it contains 0, which indicates it is not a significant predictor in the model. The result is consistent to the result from previous `lm()` table.


```{r}
beta_0 <- means[1]
beta_age <- means[2]
beta_sex_male <- means[3]
beta_bmi <- means[4]
beta_children <- means[5]
beta_region_northwest <- means[6]
beta_region_southeast <- means[7]
beta_region_northeast <- means[8]

predict_charges <- function(new_data) {
  beta_0 +
    beta_age * new_data$age +
    beta_sex_male * as.numeric(new_data$sex == 'male') +
    beta_bmi * new_data$bmi +
    beta_children * new_data$children  +
    beta_region_northwest * as.numeric(new_data$region == 'northwest') +
    beta_region_southeast * as.numeric(new_data$region == 'southeast') +
    beta_region_northeast * as.numeric(new_data$region == 'northeast')
}
```

```{r}
data_for_plot <- data.frame(
  Actual = test_set$charges,
  LR_Predicted = predict_charges(test_set)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```


The linear regression model appears predict the charges well, as indicated by the close clustering of the data points around the line of perfect prediction, where the actual value is equal to the predicted value. The data points are represented by pink dots, where the x-axis shows the actual charges and the y-axis shows the charges predicted by the model. However, for actual charges between 7.5 and 8, the model appears to overpredict, indicated by the points lying above the dashed line. As the actual charges rise from 8 to 9, the model seems to underpredict, with points lying below the dashed line. Overall, the prediction aligns well with the expected outcomes, with no apparent outlier present.
