---
title: "STAT423_Group_Project"
output: html_document
date: "2024-02-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(MASS)
library(caret)
library(faraway)
library(tidyr)
library(dplyr)
library(ggplot2)
```
# Introduction

# Data
## About the Data

```{r}
data <- read.csv("Health_insurance.csv")
```

```{r data_check}
head(data)
skimr::skim(data)
```

## Data Prepocessing
```{r}
data <- data %>%
  mutate(children = ifelse(children > 0, "have child", "no child"))
data$sex <- as.factor(data$sex)
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)
```

## Data Split (80% training 20% testing)
### (Split Before Transformation)
```{r data_split_brefore_transformation}
set.seed(423)

df_shuffled <- data[sample(nrow(data)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set1 <- df_shuffled[1:n_train, ]
test_set1 <- df_shuffled[(n_train + 1):n_total, ]
```

## Data Transformation(Log)

```{r box-cox_transform}
# Box-Cox transformation
bc_transform <- boxcox(charges ~ ., data = data, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Box-Cox transform.")

# Log transformation
data$charges <- log(data$charges)
```

## Fit Full Model

## Divide the dataset to smoker and non-smoker


# Method


```{r}
plot(train_set)
```

```{r data_check_og}
par(mfrow=c(2,2))

hist(train_set1$age, main="Age Distribution", xlab="Age")
hist(train_set1$bmi, main="BMI Distribution", xlab="BMI")
hist(train_set1$charges, main="Charges Distribution", xlab="Charges")
```
```{r lr_fit_initial}
fitfull <- lm(charges ~ ., data = train_set)
summary(fitfull)
```
```{r detailed_residual_plot}
par(mfrow=c(2, 2))

plot(fitfull,which=1)
plot(fitfull,which=2)
plot(fitfull,which=3)
plot(fitfull,which=4)
```
```{r, boxcox}
datasmoker <- train_set1 %>% filter(smoker == "yes")
datasmoker <- datasmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datasmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
datasmoker$charges <- (datasmoker$charges^lambda_optimal - 1) / lambda_optimal

test_smoker <- test_set1 %>% filter(smoker == "yes")
test_smoker <- test_smoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_smoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Smoker Box-Cox transform.")
lambda_optimal <- bc_transform$x[which.max(bc_transform$y)]
lambda_optimal <- 0.5
test_smoker$charges <- (test_smoker$charges^lambda_optimal - 1) / lambda_optimal

datanosmoker <- train_set1 %>% filter(smoker == "no")
datanosmoker <- datanosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = datanosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Train Non-Smoker Box-Cox transform.")
datanosmoker$charges <- log(datanosmoker$charges)

test_nosmoker <- test_set1 %>% filter(smoker == "no")
test_nosmoker <- test_nosmoker %>% dplyr::select(-smoker)
bc_transform <- boxcox(charges ~ ., data = test_nosmoker, lambda = seq(-2, 2, by=0.1))
title(main = "95% CI for the Test Non-Smoker Box-Cox transform.")
test_nosmoker$charges <- log(test_nosmoker$charges)

```

```{r}
fit12 <- lm(charges ~ age+sex+bmi+children+region, data = datasmoker)
summary(fit12)
par(mfrow=c(2, 2))
plot(fit12,which=1)
plot(fit12,which=2)
plot(fit12,which=3)
plot(fit12,which=4)
```

```{r}
fit13 <- lm(charges ~ age+sex+bmi+children+region, data = datanosmoker)
summary(fit13)
par(mfrow=c(2, 2))
plot(fit13,which=1)
plot(fit13,which=2)
plot(fit13,which=3)
plot(fit13,which=4)
```

## Model Selection

The Mallow’s Cp statistic, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used for the appropriate model selection. While Mallow’s Cp assesses the trade-off between model fit and complexity in linear regression, AIC and BIC extend to various modeling contexts, penalizing complexity differently based on information theory and sample size, respectively. Here, we have chosen to use these three model selection criteria for all the possible models (including the interactions of all variables) that we can build from the dataset. The surprising result is that all the optimal model selection criteria ultimately point to the same model with parameters `bmi` , `age` , and `age:bmi` interaction. The corresponding Mallow's Cp statistic, AIC, and BIC for this selected model are displayed in the following table.

```{r}
criteria_statistics <- function(mdl.list) {
  n <- nobs(mdl.list[[1]])
  DoFs <- sapply(mdl.list, function(mdl) { sum(hatvalues(mdl)) })
  MSEs <- sapply(mdl.list, function(mdl) { mean(residuals(mdl)^2) })
  AIC_values <- sapply(mdl.list, function(mdl) { AIC(mdl, k = 2) }) 
  BIC_values <- sapply(mdl.list, function(mdl) { BIC(mdl) })
  
  biggest <- which.max(DoFs)
  sigma2.hat <- MSEs[[biggest]]*n/(n-DoFs[[biggest]])
  Cp <- MSEs + 2*sigma2.hat*DoFs/n
  
  criteria_values <- cbind(Cp = Cp,
                           AIC = AIC_values,
                           BIC = BIC_values)
  return(criteria_values)
}

find_best_model <- function(criteria_values, criterion) {
  best_model_index <- which.min(criteria_values[, criterion])
  best_model <- models[[best_model_index]]
  best_combination <- combinations[[best_model_index]]
  best_criteria_value <- criteria_values[best_model_index, criterion]
  return(list(model = best_model, combination = best_combination, value = best_criteria_value))
}

get_combinations <- function(vars) {
  lapply(1:length(vars), function(x) combn(vars, x, simplify = FALSE))
}

input_vars <- names(data)[!names(data) %in% c("charges", "smoker")]

combinations <- unlist(get_combinations(input_vars), recursive = FALSE)

create_models <- function(data, combinations) {
  lapply(combinations, function(vars) {
    formula <- as.formula(paste("charges ~", paste(vars, collapse = "*")))
    lm(formula, data = datasmoker)
  })
}

models <- create_models(datasmoker, combinations)

criteria_values <- criteria_statistics(models)

best_model_Cp <- find_best_model(criteria_values, "Cp")
best_model_AIC <- find_best_model(criteria_values, "AIC")
best_model_BIC <- find_best_model(criteria_values, "BIC")

print("Best Model based on Cp:")
print(best_model_Cp$model)
print(best_model_Cp$value)
print("")

print("Best Model based on AIC:")
print(best_model_AIC$model)
print(best_model_AIC$value)
print("")

print("Best Model based on BIC:")
print(best_model_BIC$model)
print(best_model_BIC$value)
```

| Mallow's Cp| AIC | BIC |
|----------|----------|----------|
| 0.03973949 | -81.53324 | -64.65685  |

## Analysis of the selected linear model



```{r}
fit_choice <- lm(formula = charges ~ bmi + age + age:bmi, data = datasmoker)
summary(fit_choice)
```

## Assumption Checks

```{r}
par(mfrow=c(2, 2))
plot(fit_choice,which=3)
plot(fit_choice,which=2)
plot(fit_choice,which=4)
plot(fit_choice,which=5)
```

```{r}
vif(fit_choice)
```


## Prediction

```{r}
fit_choice <- lm(formula = charges ~ bmi + age + age:bmi, data = test_smoker)
data_for_plot <- data.frame(
  Actual = test_smoker$charges,
  LR_Selected_Predicted = predict(fit_choice, newdata = test_smoker)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

# Visualizing the results
ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```

For the selected model $charges=41.3066179+8.1802807bmi+1.5338854age-0.0009364bmi:age$, the only significant predictors is bmi. The predictor age and interaction term bmi:age are not statistically significant, which implies that the effect of age and the interaction effect between 'bmi' and 'age' on charges are not evident from the data. The F-statistic 220.3 with a significantly small p-value suggests that the overall model is statistically significant. The residual standard error is 33.76 and the adjusted R-squared value is 0.7537, which shows the proportion of the variance for the dependent variable explained by predictor
variables. 

The scatterplot shows the relationship between the actual charge and the charge predicted by the selected linear regression model, represented by the red dots. The data points are distributed around the dashed line, which represents the perfect prediction line, where the actual value is equal to the predicted value. The linear regression model seems to perform reasonablely well, as many of the points are clustered around the perfect fit line. However, there are significant deviations from this line, indicating some degree of prediction error. Prior to the actual charges reaching 350, the model seems to predict values that are higher than the actual figures, indicating an overestimation. Conversely, after the actual charges surpass 350, the model's predictions tend to be lower, suggesting an underestimation.


```{r}
data <- read.csv("Health_insurance.csv")
data$sex <- as.factor(data$sex)
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)

data$charges <- log(data$charges)

non_smoker_data <- filter(data, smoker == "no")

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)

threshold <- quantile(abs(residuals), 0.85)

data_with_small_residuals <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r}
par(mfrow=c(2, 2))
plot(initial_model,which=1)
plot(initial_model,which=2)
plot(initial_model,which=3)
plot(initial_model,which=4)
```

```{r}
ggplot(data_with_small_residuals, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(non_smoker_data, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()

ggplot(data_outliers, aes(x = age, y = charges)) + 
  geom_point() + 
  labs(title = "Age vs Charges", x = "Age", y = "Charges") +
  theme_minimal()
```

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = data_with_small_residuals)
```

```{r}
par(mfrow=c(2, 2))
plot(new_model,which=1)
plot(new_model,which=2)
plot(new_model,which=3)
plot(new_model,which=4)
```

```{r}
sampled_residuals <- sample(resid(new_model), replace = TRUE)
new_response <- fitted(new_model) + sampled_residuals
bootstrap_data <- data_with_small_residuals
bootstrap_data$charges <- new_response

model2<-lm(charges ~ age+sex+bmi+children+region, data = bootstrap_data)
```

```{r}
par(mfrow=c(2, 2))
plot(model2,which=1)
plot(model2,which=2)
plot(model2,which=3)
plot(model2,which=4)
```

```{r}
non_smoker_data <- filter(data, smoker == "no")

initial_model <- lm(charges ~ age + sex + bmi + children + region, data = non_smoker_data)

residuals <- resid(initial_model)
threshold <- quantile(abs(residuals), 0.85)

data_used <- non_smoker_data[abs(residuals) <= threshold, ]
data_outliers <- non_smoker_data[abs(residuals) > threshold, ]
```

```{r data_split2}
set.seed(423)

df_shuffled <- data_used[sample(nrow(data_used)), ]

n_total <- nrow(df_shuffled)
n_train <- floor(0.8 * n_total)

train_set <- df_shuffled[1:n_train, ]
test_set <- df_shuffled[(n_train + 1):n_total, ]
```

```{r}
new_model <- lm(charges ~ age + sex + bmi + children + region, data = train_set)

set.seed(423)
bootstrap_coefficients <- replicate(1000, {
  sampled_residuals <- sample(resid(new_model), replace = TRUE)
  new_response <- fitted(new_model) + sampled_residuals
  bootstrap_data <- train_set
  bootstrap_data$charges <- new_response
  coef(lm(charges ~ age + sex + bmi + children + region, data = bootstrap_data))
}, simplify = "matrix")

bootstrap_coefficients_df <- as.data.frame(t(bootstrap_coefficients))
colnames(bootstrap_coefficients_df) <- names(coef(initial_model))

bootstrap_summary <- summary(bootstrap_coefficients_df)

print(bootstrap_summary)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
ci_df <- data.frame(lower = ci_lower, upper = ci_upper)
print(ci_df)
```

```{r}
means <- colMeans(bootstrap_coefficients_df)
standard_deviations <- apply(bootstrap_coefficients_df, 2, sd)
confidence_level <- 0.95
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
ci_lower <- means - z_critical * standard_deviations
ci_upper <- means + z_critical * standard_deviations
coefficients_df <- data.frame(
  term = names(means),
  estimate = means,
  ci_lower = ci_lower,
  ci_upper = ci_upper
)

coefficients_df <- coefficients_df[-1, ]

p <- ggplot(coefficients_df, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  theme_minimal() +
  labs(x = "Term", y = "Estimate", title = "Confidence Intervals for Model Coefficients") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(p)
```

 The plot shows confidence intervals for model coefficients from a regression analysis for non-smoker data by using the bootstrap method. In the model, the predictors age and children have positive coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. While predictors regionnorthwest, regionsoutheast and regionsouthwest have negative coefficients and their confidence intervals do not include zero, suggesting they are statistically significant predictors in the model. The coefficient of bmi has small confidence interval and it contains 0, which indicates it is not a significant predictor in the model.


```{r}
beta_0 <- means[1]
beta_age <- means[2]
beta_sex_male <- means[3]
beta_bmi <- means[4]
beta_children <- means[5]
beta_region_northwest <- means[6]
beta_region_southeast <- means[7]
beta_region_northeast <- means[8]

predict_charges <- function(new_data) {
  beta_0 +
    beta_age * new_data$age +
    beta_sex_male * as.numeric(new_data$sex == 'male') +
    beta_bmi * new_data$bmi +
    beta_children * new_data$children +
    beta_region_northwest * as.numeric(new_data$region == 'northwest') +
    beta_region_southeast * as.numeric(new_data$region == 'southeast') +
    beta_region_northeast * as.numeric(new_data$region == 'northeast')
}
```


```{r}
data_for_plot2 <- data.frame(
  Actual = test_set$charges,
  LR_Predicted = predict_charges(test_set)
  ) %>% 
  pivot_longer(cols = everything(), names_to = "Model", values_to = "Predicted")

ggplot(data_for_plot2, aes(x = Predicted, fill = Model)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Predicted Charges of Testset",
       x = "Predicted Charges",
       y = "Density") +
  theme_minimal()
```

```{r}
test_smoker <- test_set %>% filter(smoker == "yes")
data_for_plot <- data.frame(
  Actual = test_set$charges,
  LR_Predicted = predict_charges(test_set)
) %>% 
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

ggplot(data_for_plot, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Model Predictions vs Actual Values in testset",
       x = "Actual Charges",
       y = "Predicted Charges") +
  theme_minimal()
```

 The linear regression model appears predict the charges well, as indicated by the close clustering of the data points around the line of perfect prediction, where the actual value is equal to the predicted value. The data points are represented by pink dots, where the x-axis shows the actual charges and the y-axis shows the charges predicted by the model. However, for actual charges between 7.5 and 8, the model appears to overpredict, indicated by the points lying above the dashed line. As the actual charges rise from 8 to 9, the model seems to underpredict, with points lying below the dashed line. Overall, the prediction aligns well with the expected outcomes, with no apparent outlier present.

